{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "require 'nn'\n",
    "require 'optim'\n",
    "require 'csvigo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded = csvigo.load('example-logistic-regression.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brands = torch.Tensor(loaded.brand)\n",
    "females = torch.Tensor(loaded.female)\n",
    "ages = torch.Tensor(loaded.age)\n",
    "dataset_inputs = torch.Tensor( (#brands)[1],2 )\n",
    "dataset_inputs[{ {},1 }] = females\n",
    "dataset_inputs[{ {},2 }] = ages\n",
    "dataset_outputs = brands\n",
    "numberOfBrands = torch.max(dataset_outputs) - torch.min(dataset_outputs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linLayer = nn.Linear(2,3)\n",
    "softMaxLayer = nn.LogSoftMax()\n",
    "model = nn.Sequential()\n",
    "model:add(linLayer)\n",
    "model:add(softMaxLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, dl_dx = model:getParameters()\n",
    "feval = function(x_new)\n",
    "   if x ~= x_new then\n",
    "      x:copy(x_new)\n",
    "   end\n",
    "\n",
    "   _nidx_ = (_nidx_ or 0) + 1\n",
    "   if _nidx_ > (#dataset_inputs)[1] then _nidx_ = 1 end\n",
    "\n",
    "   local inputs = dataset_inputs[_nidx_]\n",
    "   local target = dataset_outputs[_nidx_]\n",
    "\n",
    "   dl_dx:zero()\n",
    "\n",
    "   local loss_x = criterion:forward(model:forward(inputs), target)\n",
    "   model:backward(inputs, criterion:backward(model.output, target))\n",
    "\n",
    "   return loss_x, dl_dx\n",
    "end\n",
    "sgd_params = {\n",
    "   learningRate = 1e-3,\n",
    "   learningRateDecay = 1e-4,\n",
    "   weightDecay = 0,\n",
    "   momentum = 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1e2  -- number of times to cycle over our training data\n",
    "\n",
    "print('')\n",
    "print('============================================================')\n",
    "print('Training with SGD')\n",
    "print('')\n",
    "\n",
    "for i = 1,epochs do\n",
    "\n",
    "   current_loss = 0\n",
    "\n",
    "   for i = 1,(#dataset_inputs)[1] do\n",
    "\n",
    "      _,fs = optim.sgd(feval,x,sgd_params)\n",
    "\n",
    "      current_loss = current_loss + fs[1]\n",
    "   end\n",
    "\n",
    "   current_loss = current_loss / (#dataset_inputs)[1]\n",
    "   print('epoch = ' .. i .. \n",
    "\t ' of ' .. epochs .. \n",
    "\t ' current loss = ' .. current_loss)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model:reset()\n",
    "\n",
    "-- next we re-define the closure that evaluates f and df/dx, so that\n",
    "-- it estimates the true f, and true (exact) df/dx, over the entire\n",
    "-- dataset. This is a full batch approach.\n",
    "\n",
    "feval = function(x_new)\n",
    "   -- set x to x_new, if differnt\n",
    "   -- (in this simple example, x_new will typically always point to x,\n",
    "   -- so the copy is really useless)\n",
    "   if x ~= x_new then\n",
    "      x:copy(x_new)\n",
    "   end\n",
    "\n",
    "   -- reset gradients (gradients are always accumulated, to accomodate \n",
    "   -- batch methods)\n",
    "   dl_dx:zero()\n",
    "\n",
    "   -- and batch over the whole training dataset:\n",
    "   local loss_x = 0\n",
    "   for i = 1,(#dataset_inputs)[1] do\n",
    "      -- select a new training sample\n",
    "      _nidx_ = (_nidx_ or 0) + 1\n",
    "      if _nidx_ > (#dataset_inputs)[1] then _nidx_ = 1 end\n",
    "\n",
    "      local inputs = dataset_inputs[_nidx_]\n",
    "      local target = dataset_outputs[_nidx_]\n",
    "\n",
    "      -- evaluate the loss function and its derivative wrt x, for that sample\n",
    "      loss_x = loss_x + criterion:forward(model:forward(inputs), target)\n",
    "      model:backward(inputs, criterion:backward(model.output, target))\n",
    "   end\n",
    "\n",
    "   -- normalize with batch size\n",
    "   loss_x = loss_x / (#dataset_inputs)[1]\n",
    "   dl_dx = dl_dx:div( (#dataset_inputs)[1] )\n",
    "\n",
    "   -- return loss(x) and dloss/dx\n",
    "   return loss_x, dl_dx\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lbfgs_params = {\n",
    "   lineSearch = optim.lswolfe,\n",
    "   maxIter = epochs,\n",
    "   verbose = true\n",
    "}\n",
    "\n",
    "print('')\n",
    "print('============================================================')\n",
    "print('Training with L-BFGS')\n",
    "print('')\n",
    "\n",
    "_,fs = optim.lbfgs(feval,x,lbfgs_params)\n",
    "\n",
    "-- fs contains all the evaluations of f, during optimization\n",
    "\n",
    "print('history of L-BFGS evaluations:')\n",
    "print(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('')\n",
    "print('============================================================')\n",
    "print('Testing the model')\n",
    "print('')\n",
    "\n",
    "-- Now that the model is trained, one can test it by evaluating it\n",
    "-- on new samples.\n",
    "\n",
    "-- The model constructed and trained above computes the probabilities\n",
    "-- of each class given the input values.\n",
    "\n",
    "-- We want to compare our model's results with those from the text.\n",
    "-- The input variables have narrow ranges, so we just compare all possible\n",
    "-- input variables in the training data.\n",
    "\n",
    "-- Determine actual frequency of the each female-age pair in the \n",
    "-- training data\n",
    "\n",
    "-- return index of largest value\n",
    "function maxIndex(a,b,c)\n",
    "   if a >=b and a >= c then return 1 \n",
    "   elseif b >= a and b >= c then return 2\n",
    "   else return 3 end\n",
    "end\n",
    "\n",
    "-- return predicted brand and probabilities of each brand\n",
    "-- for the model in the text\n",
    "\n",
    "-- The R code in the text computes the probabilities of choosing\n",
    "-- brands 2 and 3 relative to the probability of choosing brand 1:\n",
    "--   Prob(brand=2)/prob(brand=1) = exp(-11.77 + 0.52*female + 0.37*age)\n",
    "--   Prob(brand=3)/prob(brand=1) = exp(-22.72 + 0.47*female + 0.69*age)\n",
    "function predictText(age, female)\n",
    "   --   1: calculate the \"logit's\"\n",
    "   --      The coefficients come from the text.\n",
    "   --      If you download the R script and run it, you may see slightly\n",
    "   --      different results.\n",
    "   local logit1 = 0\n",
    "   local logit2 = -11.774655 + 0.523814 * female + 0.368206 * age\n",
    "   local logit3 = -22.721396 + 0.465941 * female + 0.685908 * age\n",
    "\n",
    "   --   2: calculate the unnormalized probabilities\n",
    "   local uprob1 = math.exp(logit1)\n",
    "   local uprob2 = math.exp(logit2)\n",
    "   local uprob3 = math.exp(logit3)\n",
    "\n",
    "   --   3: normalize the probabilities\n",
    "   local z = uprob1 + uprob2 + uprob3\n",
    "   local prob1 = (1/z) * uprob1\n",
    "   local prob2 = (1/z) * uprob2\n",
    "   local prob3 = (1/z) * uprob3\n",
    "\n",
    "   return maxIndex(prob1, prob2, prob3), prob1, prob2, prob3\n",
    "end\n",
    "\n",
    "-- return predicted brand and the probabilities of each brand\n",
    "-- for our model\n",
    "function predictOur(age, female)\n",
    "   local input = torch.Tensor(2)\n",
    "   input[1] = female  -- must be in same order as when the model was trained!\n",
    "   input[2] = age\n",
    "   local logProbs = model:forward(input)  \n",
    "   --print('predictOur', age, female, input)\n",
    "   local probs = torch.exp(logProbs)\n",
    "   --print('logProbs', logProbs)\n",
    "   --print('probs', probs[1], probs[2], probs[3] )\n",
    "   local prob1, prob2, prob3 = probs[1], probs[2], probs[3]\n",
    "   return maxIndex(prob1, prob2, prob3), prob1, prob2, prob3\n",
    "end\n",
    "      \n",
    "counts = {}\n",
    "\n",
    "function makeKey(age, brand, female)\n",
    "   -- return a string containing the values\n",
    "\n",
    "   -- Note that returning a table will not work, because each\n",
    "   -- table is unique.\n",
    "\n",
    "   -- Because Lua interns the strings, a string with a given sequence\n",
    "   -- of characters is stored only once.\n",
    "   return string.format('%2d%1d%1f', age, brand, female)\n",
    "end\n",
    "\n",
    "for i = 1,(#brands)[1] do\n",
    "   local brand = brands[i]\n",
    "   local female = females[i]\n",
    "   local age = ages[i]\n",
    "   local key = makeKey (age, brand, female)\n",
    "   counts[key] = (counts[key] or 0) + 1\n",
    "end\n",
    "\n",
    "-- return probability of each brand conditioned on age and female\n",
    "function actualProbabilities(age, female)\n",
    "   function countOf(age, brand, female)\n",
    "      return counts[makeKey(age, brand, female)] or 0\n",
    "   end\n",
    "   local count1 = countOf(age, 1, female)\n",
    "   local count2 = countOf(age, 2, female)\n",
    "   local count3 = countOf(age, 3, female)\n",
    "   local sumCounts = count1 + count2 + count3\n",
    "   if sumCounts == 0 then\n",
    "      return 0, 0, 0\n",
    "   else\n",
    "      return count1/sumCounts, count2/sumCounts, count3/sumCounts\n",
    "   end\n",
    "end\n",
    "\n",
    "\n",
    "print(' ')\n",
    "print('summary of data')\n",
    "summarizeData()\n",
    "\n",
    "print(' ')\n",
    "print('training variables')\n",
    "for k,v in pairs(sgd_params) do\n",
    "   print(string.format('%20s %f', k, v))\n",
    "end\n",
    "print(string.format('%20s %f', 'epochs', epochs))\n",
    "\n",
    "print(' ')\n",
    "print('current loss', current_loss)\n",
    "\n",
    "-- print the headers \n",
    "print(' ')\n",
    "lineFormat = '%-6s %-3s| %-17s | %-17s | %-17s | %-1s %-1s %-1s'\n",
    "print(\n",
    "   string.format(lineFormat,\n",
    "\t\t '', '', \n",
    "\t\t 'actual probs', 'text probs', 'our probs', \n",
    "\t\t 'best', '', ''))\n",
    "choices = 'brnd1 brnd2 brnd3'\n",
    "print(string.format(lineFormat,\n",
    "\t\t    'female', 'age', \n",
    "\t\t    choices, choices, choices, \n",
    "\t\t    'a', 't', 'o'))\n",
    "\n",
    "-- print each row in the table\n",
    "\n",
    "function formatFemale(female)\n",
    "   return string.format('%1d', female)\n",
    "end\n",
    "\n",
    "function formatAge(age)\n",
    "   return string.format('%2d', age)\n",
    "end\n",
    "\n",
    "function formatProbs(p1, p2, p3)\n",
    "   return string.format('%5.3f %5.3f %5.3f', p1, p2, p3)\n",
    "end\n",
    "\n",
    "function indexString(p1, p2, p3)\n",
    "   -- return index of highest probability or '-' if nearly all zeroes\n",
    "   if p1 < 0.001 and p2 < 0.001 and p3 < 0.001 then\n",
    "      return '-'\n",
    "   else \n",
    "      return string.format('%1d', maxIndex(p1, p2, p3))\n",
    "   end\n",
    "end\n",
    "\n",
    "-- print table rows and accumulate accuracy\n",
    "for female = 0,1 do\n",
    "   for age = torch.min(ages),torch.max(ages) do\n",
    "      -- calculate the actual probabilities in the training data\n",
    "      local actual1, actual2, actual3 = actualProbabilities(age, female)\n",
    "      -- calculate the prediction and probabilities using the model in the text\n",
    "      local textBrand, textProb1, textProb2, textProb3 = \n",
    "\t predictText(age, female)\n",
    "      -- calculate the probabilities using the model we just trained\n",
    "      --print(\"main\", age, female)\n",
    "      local ourBrand, ourProb1, ourProb2, ourProb3 = \n",
    "\t predictOur(age, female)\n",
    "      print(\n",
    "\t string.format(lineFormat,\n",
    "\t\t       formatFemale(female), \n",
    "\t\t       formatAge(age),\n",
    "\t\t       formatProbs(actual1, actual2, actual3),\n",
    "\t\t       formatProbs(textProb1, textProb2, textProb3),\n",
    "\t\t       formatProbs(ourProb1, ourProb2, ourProb3),\n",
    "\t\t       indexString(actual1,actual2,actual3),\n",
    "\t\t       indexString(textProb1,textProb2,textProb3),\n",
    "\t\t       indexString(ourProb1,ourProb2,ourProb3))\n",
    "\t   )\n",
    "   end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
